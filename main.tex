\documentclass{article}
\title{Problem set, MT5017}
\author{Ville Sebastian Olsson}
\usepackage[a4paper,margin=2em]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage[parfill]{parskip}
\usepackage{hyperref} % Clickable ToC
\hypersetup{colorlinks}

\usepackage{sebelino-mathlib} % Custom sty file
\setcounter{secnumdepth}{0}
\begin{document}
\maketitle
\tableofcontents

\section{Bonus exercises}
\subsection{Exercise 4}

Let \(x_1,\ldots,x_n\) be a sample (measured in units of mm) of independent
realizations of \(X\) with density \(f_X(\cdot |\theta)\) and let \(y_1, \ldots , y_n\) be the same sample in units of m. Find the likelihood ratio
\[\frac{L_X(\theta|x)}{L_Y(\theta|y)}\]

\textbf{Solution:}

Since \(x=(x_1,\ldots,x_n)\) are independent realizations of \(X\), and \(\forall k: y_k=\frac{1}{1000}x_k\), we can consider \(y=(y_1,\ldots, y_n)\) to be independent realizations of a random variable \(Y=h(X)\) where
\(h(t)=\frac{1}{1000}t\).

Then \(h^{-1}(t) = 1000t\), and the ratio is calculated as follows:
\begin{align*}
     & \frac{L_X(\theta|x)}{L_Y(\theta|y)} \\
    =& \frac{f_X(x|\theta)}{f_Y(y|\theta)} \\
    =& \frac{f_{X_{1:n}}(x_1,\ldots, x_n|\theta)}{f_{Y_{1:n}}(y_1\ldots, y_n|\theta)} \\
    =& \frac{\prod_{k=1}^n f_{X_k}(x_k|\theta)}{f_{Y_{1:n}}(y_1,\ldots,y_n|\theta)} & (\text{since }X_1,\ldots X_n\text{ independent}) \\
    =& \frac{\prod_{k=1}^n f_{X_k}(x_k|\theta)}{\prod_{k=1}^n f_{Y_k}(y_k|\theta)} & (\text{since }Y_1,\ldots Y_n\text{ independent}) \\
    =& \frac{\prod_{k=1}^n f_{X_k}(x_k|\theta)}{\prod_{k=1}^n \left( f_{X_k}(h^{-1}(y_k)|\theta) \cdot |\frac{d}{dy}h^{-1}(y)| \right)} & (\text{Transformation theorem}) \\
    =& \frac{\prod_{k=1}^n f_{X_k}(x_k|\theta)}{\prod_{k=1}^n f_{X_k}(h^{-1}(y_k)|\theta) \cdot \prod_{k=1}^n |\frac{d}{dy}h^{-1}(y)|} \\
    =& \frac{\prod_{k=1}^n f_{X_k}(x_k|\theta)}{\prod_{k=1}^n f_{X_k}(1000y_k|\theta) \cdot \prod_{k=1}^n |\frac{d}{dy}1000y|} \\
    =& \frac{\prod_{k=1}^n f_{X_k}(x_k|\theta)}{\prod_{k=1}^n f_{X_k}(1000y_k|\theta) \cdot \prod_{k=1}^n 1000} \\
    =& \frac{\prod_{k=1}^n f_{X_k}(x_k|\theta)}{\prod_{k=1}^n f_{X_k}(x_k|\theta) \cdot \prod_{k=1}^n 1000} \\
    =& \frac{1}{\prod_{k=1}^n 1000} \\
    =& \frac{1}{1000^n} \\
\end{align*}

\section{Misc problems}
\subsection{Boxes with chips}

Two boxes, A and B.
One box contains 50 \% defective chips. Another box contains 10 \% defective chips.
You pick 10 chips from box A. The first 3 chips are defective while the rest are fine.

Which box do you think contains the 10 \% defective chips: A or B?

\textbf{Solution:}

Let \(X\) be a random variable such that \(X=1\) if you pick a defective chip from box A, and \(X=0\) otherwise.

In one box, 10 \% are defective. In the other, 50 \% are defective. Therefore, \(X\) is Bernoulli-distributed:
\[X \sim \text{Ber}(\theta)\]
with PMF:
\[p_X(x) = \casesiii{\theta}{x=1}{1-\theta}{x=0}{0}\]
where \(\theta=\frac{1}{10} \vee \theta=\frac{1}{2}\).

You devise an experiment where you pick 10 chips from box A.

Your experiment will involve a \textit{random sample} which is a collection of 10 random variables: \(X_1,\ldots,X_{10}\).

When you perform the experiment, a \textit{realization} of the random sample is created: \(x_1, \ldots, x_{10}\).

In this case, the first 3 chips are defective:
\[(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10})=(1,1,1,0,0,0,0,0,0,0)\]

\begin{align*}
     & P(X_1=1\cap X_2=1 \cap X_3=1\cap X_4=0 \cap\ldots \cap X_{10}=0) \\
    =& P(X_1=1)P(X_2=1)P(X_3=1)\cdot P(X_4=0) \cdots P(X_{10}=0) & (X_1,\ldots,X_{10}\text{ independent}) \\
    =& P(X=0)^9\cdot P(X=1) & (X_1,\ldots,X_{10}\text{ identically distributed}) \\
    =& p_X(0)^7\cdot p_X(1)^3 \\
    =& (1-\theta)^7\theta^3 \\
    =& \casesiie{(1-\frac{1}{10})^7\cdot (\frac{1}{10})^3}{\theta=\frac{1}{10}}{(1-\frac{1}{2})^7\cdot (\frac{1}{2})^3}{\theta=\frac{1}{2}} \\
    =& \casesiie{0.048 \%}{\theta=\frac{1}{10}}{0.098 \%}{\theta=\frac{1}{2}} \\
    =& L(\theta)
\end{align*}
The realization is more probable if \(\theta=\frac{1}{2}\), so the likelihood of \(\theta=\frac{1}{2}\) is greater.

Equivalently, \(L(\frac{1}{2}) > L(\frac{1}{10})\).

Therefore, box A is probably the box with 50 \% defective chips in it.

Furthermore, if nothing was known about the boxes, then
\[\hat{\theta} = \frac{\bar{x}_n}{n} = \frac{3}{10}\]
would be the maximum-likelihood estimate of \(\theta\).

\subsection{Likelihood}

Let \(X\sim \operatorname{Bern}(\theta)\).

a)
Consider a random sample r.v. of size 1, i.e. \(X_1\).
Consider a realization of this sample, i.e. \(x_1\).

What is the likelihood function for this sample?

\textbf{Solution:}

\(X\) has a PMF:
\[p_{\theta;X}(x) = \casesiie{\theta}{x=1}{1-\theta}{x=0}\]

\begin{align*}
     & L(\theta;x_1) \\
    =& p_{\theta;X_1}(x_1) \\
    =& p_{\theta;X}(x_1) & (X_1,X\text{ identically distributed}) \\
    =& \casesiie{\theta}{x_1=1}{1-\theta}{x_1=0} \\
    =& \theta^{x_1}(1-\theta)^{1-x_1}
\end{align*}

b)
Consider a random sample r.v. of size 2, i.e. \(X_1, X_2\).
Consider a realization of this sample, i.e. \(x_1, x_2\).

What is the likelihood function for this sample?

\textbf{Solution:}

\begin{align*}
     & L(\theta;x_1,x_2) \\
    =& p_{\theta;X_1,X_2}(x_1,x_2) \\
    =& p_{\theta;X_1}(x_1)\cdot p_{\theta;X_2}(x_2) & (X_1,X_2\text{ independent}) \\
    =& p_{\theta;X}(x_1)\cdot p_{\theta;X}(x_2) & (X_1,X_2\text{ identically distributed}) \\
    =& \prod_{k=1}^2 p_{\theta;X}(x_k) \\
    =& \prod_{k=1}^2 \casesiie{\theta}{x=1}{1-\theta}{x=0} \\
    =& \casesiiie{\theta^2}{(x_1,x_2)=(1,1)}{\theta(1-\theta)}{(x_1,x_2)\in\{(1,0),(0,1)\}}{(1-\theta)^2}{(x_1,x_2)=(0,0)} \\
    =& \theta^{x_1+x_2}(1-\theta)^{2-(x_1+x_2)}
\end{align*}

c)
Consider a random sample r.v. of size n, i.e. \(X_{1:n}=(X_1, \ldots, X_n)\).
Consider a realization of this sample, i.e. \(x=(x_1, \ldots, x_n)\).

What is the likelihood function for this sample?

\textbf{Solution:}

\begin{align*}
     & L(\theta;x) \\
    =& L(\theta;x_1,\ldots,x_n) \\
    =& p_{\theta;X_{1:n}}(x_1,\ldots,x_n) \\
    =& p_{\theta;X_1,\ldots,X_n}(x_1,\ldots,x_n) \\
    =& p_{\theta;X_1}(x_1)\cdot \ldots \cdot p_{\theta;X_n}(x_n) & (X_1,\ldots,X_n\text{ independent}) \\
    =& p_{\theta;X}(x_1)\cdot \ldots \cdot p_{\theta;X}(x_n) & (X_1,\ldots,X_n\text{ identically distributed}) \\
    =& \prod_{k=1}^n p_{\theta;X}(x_k) \\
    =& \prod_{k=1}^n \casesiie{\theta}{x=1}{1-\theta}{x=0} \\
    =& \theta^{\sum_{k=1}^n x_k}\cdot (1-\theta)^{n-\sum_{k=1}^n x_k} \\
    =& \theta^{\bar{x}} (1-\theta)^{n-\bar{x}}
\end{align*}
where \(\bar{x}=\sum_{k=1}^n x_k=x_1+\ldots+x_n\).
\end{document}
